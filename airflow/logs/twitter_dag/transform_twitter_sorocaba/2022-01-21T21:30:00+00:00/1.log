[2022-01-22 18:31:51,843] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_sorocaba 2022-01-21T21:30:00+00:00 [queued]>
[2022-01-22 18:31:51,852] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_sorocaba 2022-01-21T21:30:00+00:00 [queued]>
[2022-01-22 18:31:51,852] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-01-22 18:31:51,852] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-01-22 18:31:51,852] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-01-22 18:31:51,867] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_sorocaba> on 2022-01-21T21:30:00+00:00
[2022-01-22 18:31:51,869] {standard_task_runner.py:54} INFO - Started process 9195 to run task
[2022-01-22 18:31:51,916] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twitter_sorocaba', '2022-01-21T21:30:00+00:00', '--job_id', '18', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmpxqucnclt']
[2022-01-22 18:31:51,917] {standard_task_runner.py:78} INFO - Job 18: Subtask transform_twitter_sorocaba
[2022-01-22 18:31:51,940] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twitter_sorocaba 2022-01-21T21:30:00+00:00 [running]> on host nicholas-VirtualBox-ubuntu1804
[2022-01-22 18:31:51,981] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-01-22 18:31:51,983] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/nicholas/Downloads/spark-3.2.0-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/nicholas/datapipeline/spark/transformation.py --src /home/nicholas/Documents/alura/datapipeline/datalake/bronze/twitter_sorocaba/extract_date=2022-01-21 --dest /home/nicholas/Documents/alura/datapipeline/datalake/silver/twitter_sorocaba/ --process-date 2022-01-21
[2022-01-22 18:31:54,757] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:54 WARN Utils: Your hostname, nicholas-VirtualBox-ubuntu1804 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2022-01-22 18:31:54,758] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-01-22 18:31:56,504] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-01-22 18:31:56,521] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO SparkContext: Running Spark version 3.2.0
[2022-01-22 18:31:56,748] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-01-22 18:31:56,935] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceUtils: ==============================================================
[2022-01-22 18:31:56,936] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-01-22 18:31:56,936] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceUtils: ==============================================================
[2022-01-22 18:31:56,937] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO SparkContext: Submitted application: twitter_transformation
[2022-01-22 18:31:56,974] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-01-22 18:31:56,990] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceProfile: Limiting resource is cpu
[2022-01-22 18:31:56,991] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-01-22 18:31:57,084] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SecurityManager: Changing view acls to: nicholas
[2022-01-22 18:31:57,084] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SecurityManager: Changing modify acls to: nicholas
[2022-01-22 18:31:57,085] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SecurityManager: Changing view acls groups to:
[2022-01-22 18:31:57,085] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SecurityManager: Changing modify acls groups to:
[2022-01-22 18:31:57,086] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nicholas); groups with view permissions: Set(); users  with modify permissions: Set(nicholas); groups with modify permissions: Set()
[2022-01-22 18:31:57,493] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO Utils: Successfully started service 'sparkDriver' on port 35949.
[2022-01-22 18:31:57,529] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SparkEnv: Registering MapOutputTracker
[2022-01-22 18:31:57,566] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SparkEnv: Registering BlockManagerMaster
[2022-01-22 18:31:57,591] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-01-22 18:31:57,592] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-01-22 18:31:57,598] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-01-22 18:31:57,652] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-25aea200-925f-4cf3-b7e9-52a7be8f1235
[2022-01-22 18:31:57,673] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-01-22 18:31:57,694] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:57 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-01-22 18:31:58,045] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-01-22 18:31:58,114] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
[2022-01-22 18:31:58,386] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO Executor: Starting executor ID driver on host 10.0.2.15
[2022-01-22 18:31:58,416] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34729.
[2022-01-22 18:31:58,417] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO NettyBlockTransferService: Server created on 10.0.2.15:34729
[2022-01-22 18:31:58,420] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-01-22 18:31:58,429] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34729, None)
[2022-01-22 18:31:58,445] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34729 with 366.3 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34729, None)
[2022-01-22 18:31:58,448] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34729, None)
[2022-01-22 18:31:58,449] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34729, None)
[2022-01-22 18:31:58,702] {spark_submit_hook.py:479} INFO - /home/nicholas/Downloads/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
[2022-01-22 18:31:58,703] {spark_submit_hook.py:479} INFO - FutureWarning
[2022-01-22 18:31:59,132] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-01-22 18:31:59,169] {spark_submit_hook.py:479} INFO - 22/01/22 18:31:59 INFO SharedState: Warehouse path is 'file:/home/nicholas/datapipeline/spark-warehouse'.
[2022-01-22 18:32:00,649] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:00 INFO InMemoryFileIndex: It took 102 ms to list leaf files for 1 paths.
[2022-01-22 18:32:00,849] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2022-01-22 18:32:03,221] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO FileSourceStrategy: Pushed Filters:
[2022-01-22 18:32:03,222] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO FileSourceStrategy: Post-Scan Filters:
[2022-01-22 18:32:03,225] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-01-22 18:32:03,623] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-01-22 18:32:03,716] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-01-22 18:32:03,719] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34729 (size: 32.5 KiB, free: 366.3 MiB)
[2022-01-22 18:32:03,724] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:03,736] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4545115 bytes, open cost is considered as scanning 4194304 bytes.
[2022-01-22 18:32:03,947] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:03,965] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-01-22 18:32:03,966] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-01-22 18:32:03,966] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO DAGScheduler: Parents of final stage: List()
[2022-01-22 18:32:03,967] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO DAGScheduler: Missing parents: List()
[2022-01-22 18:32:03,972] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-01-22 18:32:04,077] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-01-22 18:32:04,080] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-01-22 18:32:04,080] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34729 (size: 6.5 KiB, free: 366.3 MiB)
[2022-01-22 18:32:04,081] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427
[2022-01-22 18:32:04,095] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-01-22 18:32:04,096] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-01-22 18:32:04,156] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 4960 bytes) taskResourceAssignments Map()
[2022-01-22 18:32:04,177] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-01-22 18:32:04,606] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO FileScanRDD: Reading File path: file:///home/nicholas/Documents/alura/datapipeline/datalake/bronze/twitter_sorocaba/extract_date=2022-01-21/QuerySorocaba_20220121.json, range: 0-350811, partition values: [empty row]
[2022-01-22 18:32:04,836] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO CodeGenerator: Code generated in 193.771493 ms
[2022-01-22 18:32:04,959] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2858 bytes result sent to driver
[2022-01-22 18:32:04,967] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 825 ms on 10.0.2.15 (executor driver) (1/1)
[2022-01-22 18:32:04,970] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-01-22 18:32:04,983] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,986 s
[2022-01-22 18:32:04,986] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-01-22 18:32:04,986] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-01-22 18:32:04,994] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,043110 s
[2022-01-22 18:32:05,496] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2022-01-22 18:32:05,498] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2022-01-22 18:32:05,498] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,id:string,in_reply_to_user_id:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2022-01-22 18:32:05,617] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-01-22 18:32:05,617] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-01-22 18:32:05,619] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-01-22 18:32:05,832] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO CodeGenerator: Code generated in 73.657512 ms
[2022-01-22 18:32:05,836] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-01-22 18:32:05,845] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 365.6 MiB)
[2022-01-22 18:32:05,846] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34729 (size: 32.4 KiB, free: 366.2 MiB)
[2022-01-22 18:32:05,847] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:05,849] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4545115 bytes, open cost is considered as scanning 4194304 bytes.
[2022-01-22 18:32:05,916] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:05,917] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-01-22 18:32:05,917] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-01-22 18:32:05,917] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Parents of final stage: List()
[2022-01-22 18:32:05,917] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Missing parents: List()
[2022-01-22 18:32:05,918] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-01-22 18:32:05,945] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 223.5 KiB, free 365.3 MiB)
[2022-01-22 18:32:05,948] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 76.8 KiB, free 365.3 MiB)
[2022-01-22 18:32:05,948] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34729 (size: 76.8 KiB, free: 366.2 MiB)
[2022-01-22 18:32:05,949] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1427
[2022-01-22 18:32:05,950] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-01-22 18:32:05,950] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-01-22 18:32:05,958] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 5189 bytes) taskResourceAssignments Map()
[2022-01-22 18:32:05,958] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-01-22 18:32:06,037] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-01-22 18:32:06,037] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-01-22 18:32:06,038] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-01-22 18:32:06,121] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileScanRDD: Reading File path: file:///home/nicholas/Documents/alura/datapipeline/datalake/bronze/twitter_sorocaba/extract_date=2022-01-21/QuerySorocaba_20220121.json, range: 0-350811, partition values: [empty row]
[2022-01-22 18:32:06,144] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO CodeGenerator: Code generated in 19.698479 ms
[2022-01-22 18:32:06,181] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO CodeGenerator: Code generated in 5.307824 ms
[2022-01-22 18:32:06,284] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: Saved output of task 'attempt_20220122183205159513247886379753_0001_m_000000_1' to file:/home/nicholas/Documents/alura/datapipeline/datalake/silver/twitter_sorocaba/tweet/process_date=2022-01-21/_temporary/0/task_20220122183205159513247886379753_0001_m_000000
[2022-01-22 18:32:06,287] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkHadoopMapRedUtil: attempt_20220122183205159513247886379753_0001_m_000000_1: Committed
[2022-01-22 18:32:06,303] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2713 bytes result sent to driver
[2022-01-22 18:32:06,311] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 356 ms on 10.0.2.15 (executor driver) (1/1)
[2022-01-22 18:32:06,318] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,394 s
[2022-01-22 18:32:06,319] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-01-22 18:32:06,321] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-01-22 18:32:06,325] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-01-22 18:32:06,326] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,410364 s
[2022-01-22 18:32:06,330] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Start to commit write Job 252b9130-e3bf-40ba-8605-b28ecb16846f.
[2022-01-22 18:32:06,364] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Write Job 252b9130-e3bf-40ba-8605-b28ecb16846f committed. Elapsed time: 31 ms.
[2022-01-22 18:32:06,372] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Finished processing stats for write job 252b9130-e3bf-40ba-8605-b28ecb16846f.
[2022-01-22 18:32:06,455] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(includes)
[2022-01-22 18:32:06,455] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(includes#8),(size(includes#8.users, true) > 0),isnotnull(includes#8.users)
[2022-01-22 18:32:06,456] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2022-01-22 18:32:06,471] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-01-22 18:32:06,471] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-01-22 18:32:06,471] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-01-22 18:32:06,535] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO CodeGenerator: Code generated in 24.17442 ms
[2022-01-22 18:32:06,541] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 337.9 KiB, free 364.9 MiB)
[2022-01-22 18:32:06,553] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 364.9 MiB)
[2022-01-22 18:32:06,554] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:34729 (size: 32.4 KiB, free: 366.1 MiB)
[2022-01-22 18:32:06,555] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:06,556] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4545115 bytes, open cost is considered as scanning 4194304 bytes.
[2022-01-22 18:32:06,583] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-01-22 18:32:06,586] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-01-22 18:32:06,586] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-01-22 18:32:06,586] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Parents of final stage: List()
[2022-01-22 18:32:06,586] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Missing parents: List()
[2022-01-22 18:32:06,588] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-01-22 18:32:06,610] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.1 KiB, free 364.7 MiB)
[2022-01-22 18:32:06,613] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 75.2 KiB, free 364.6 MiB)
[2022-01-22 18:32:06,614] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:34729 (size: 75.2 KiB, free: 366.1 MiB)
[2022-01-22 18:32:06,614] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1427
[2022-01-22 18:32:06,615] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-01-22 18:32:06,615] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-01-22 18:32:06,617] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 5189 bytes) taskResourceAssignments Map()
[2022-01-22 18:32:06,617] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-01-22 18:32:06,633] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-01-22 18:32:06,633] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-01-22 18:32:06,633] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-01-22 18:32:06,651] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileScanRDD: Reading File path: file:///home/nicholas/Documents/alura/datapipeline/datalake/bronze/twitter_sorocaba/extract_date=2022-01-21/QuerySorocaba_20220121.json, range: 0-350811, partition values: [empty row]
[2022-01-22 18:32:06,664] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO CodeGenerator: Code generated in 10.965805 ms
[2022-01-22 18:32:06,671] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO CodeGenerator: Code generated in 4.851595 ms
[2022-01-22 18:32:06,694] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileOutputCommitter: Saved output of task 'attempt_202201221832061783577293296867888_0002_m_000000_2' to file:/home/nicholas/Documents/alura/datapipeline/datalake/silver/twitter_sorocaba/user/process_date=2022-01-21/_temporary/0/task_202201221832061783577293296867888_0002_m_000000
[2022-01-22 18:32:06,694] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkHadoopMapRedUtil: attempt_202201221832061783577293296867888_0002_m_000000_2: Committed
[2022-01-22 18:32:06,695] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2627 bytes result sent to driver
[2022-01-22 18:32:06,697] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on 10.0.2.15 (executor driver) (1/1)
[2022-01-22 18:32:06,699] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,111 s
[2022-01-22 18:32:06,699] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-01-22 18:32:06,699] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-01-22 18:32:06,700] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-01-22 18:32:06,700] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,116219 s
[2022-01-22 18:32:06,700] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Start to commit write Job e2bdb793-343b-4b2a-bfbc-30f486167941.
[2022-01-22 18:32:06,713] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Write Job e2bdb793-343b-4b2a-bfbc-30f486167941 committed. Elapsed time: 12 ms.
[2022-01-22 18:32:06,713] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO FileFormatWriter: Finished processing stats for write job e2bdb793-343b-4b2a-bfbc-30f486167941.
[2022-01-22 18:32:06,768] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkContext: Invoking stop() from shutdown hook
[2022-01-22 18:32:06,782] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
[2022-01-22 18:32:06,799] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-01-22 18:32:06,833] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO MemoryStore: MemoryStore cleared
[2022-01-22 18:32:06,835] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO BlockManager: BlockManager stopped
[2022-01-22 18:32:06,849] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-01-22 18:32:06,854] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-01-22 18:32:06,863] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO SparkContext: Successfully stopped SparkContext
[2022-01-22 18:32:06,864] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO ShutdownHookManager: Shutdown hook called
[2022-01-22 18:32:06,864] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ed6721b-922b-4904-86bc-ebc7a13aab9a/pyspark-147b45f5-1c88-4757-b132-bafdf709281d
[2022-01-22 18:32:06,869] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0bde9d2-cf42-4b45-9006-05feba491c20
[2022-01-22 18:32:06,872] {spark_submit_hook.py:479} INFO - 22/01/22 18:32:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ed6721b-922b-4904-86bc-ebc7a13aab9a
[2022-01-22 18:32:06,965] {taskinstance.py:1070} INFO - Marking task as SUCCESS.dag_id=twitter_dag, task_id=transform_twitter_sorocaba, execution_date=20220121T213000, start_date=20220122T213151, end_date=20220122T213206
[2022-01-22 18:32:11,876] {local_task_job.py:102} INFO - Task exited with return code 0
